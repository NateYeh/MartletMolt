# ä»»å‹™äº¤æ¥ï¼šä¸²æµç·©è¡æ¶æ§‹æ”¹é€²ï¼ˆæ–¹æ¡ˆ2ï¼šæ··åˆæ¨¡å¼ï¼‰

> **ç‹€æ…‹ï¼šå·²å®Œæˆ** âœ…
> 
> **å®Œæˆæ—¥æœŸ**ï¼š2025-02-22

> **ä»»å‹™ç›®æ¨™**ï¼šå¯¦ç¾å¾Œç«¯ç¨ç«‹å®Œæˆ OpenAI ä¸²æµï¼Œå‰ç«¯æ–·ç·šä¸å½±éŸ¿å¾Œå°è™•ç†

---

## ä¸€ã€ä»»å‹™æ¦‚è¿°

### 1.1 å•é¡Œæè¿°

**ç•¶å‰æ¶æ§‹å•é¡Œï¼š**

```
ç•¶å‰æµç¨‹ï¼š
å‰ç«¯ â†” å¾Œç«¯ â†” OpenAI API
      â†‘ ä¸‰è€…ä¸²æµç¶åœ¨ä¸€èµ·

å•é¡Œï¼š
âŒ å‰ç«¯æ–·ç·š â†’ å¾Œç«¯ä¸²æµç«‹å³ä¸­æ–· â†’ OpenAI API åœæ­¢
âŒ æœƒè©±æ­·å²ä¸Ÿå¤±ï¼ˆAI å›æ‡‰æœªä¿å­˜ï¼‰
âŒ æµªè²» OpenAI API tokensï¼ˆå·²ç”Ÿæˆä½†ä¸­æ–·ï¼‰
```

### 1.2 æ”¹é€²ç›®æ¨™

**æ–°æ¶æ§‹ï¼š**

```
æ”¹é€²æµç¨‹ï¼š
OpenAIä¸²æµ â”€â”€â†’ å¾Œç«¯ç·©è¡å€ â”€â”€â†’ å‰ç«¯æ¶ˆè²»
                 â†“
              å¾Œå°ä¿å­˜

å„ªé»ï¼š
âœ… å¾Œç«¯ç¨ç«‹å®Œæˆ OpenAI ä¸²æµ
âœ… å‰ç«¯æ–·ç·šä¸å½±éŸ¿å¾Œå°è™•ç†
âœ… å®Œæ•´å›æ‡‰å¿…å®šä¿å­˜
âœ… ä¿æŒå³æ™‚ä¸²æµé«”é©—
```

---

## äºŒã€æ¶æ§‹è¨­è¨ˆ

### 2.1 æ ¸å¿ƒæ¦‚å¿µ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Endpoint: /chat/stream (æ”¹é€²ç‰ˆ)                 â”‚
â”‚                                                          â”‚
â”‚  1. æ¥æ”¶è«‹æ±‚                                              â”‚
â”‚     POST /chat/stream                                    â”‚
â”‚     {"message": "ä½ å¥½", "session_id": "abc"}             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. å‰µå»ºç·©è¡å€ï¼ˆasyncio.Queueï¼‰                           â”‚
â”‚     buffer = StreamBuffer()                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. å•Ÿå‹•å¾Œå°ä»»å‹™ï¼ˆç¨ç«‹é‹è¡Œï¼‰                               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚     â”‚ async def process_openai_stream():          â”‚      â”‚
â”‚     â”‚   - èª¿ç”¨ OpenAI API (stream=True)           â”‚      â”‚
â”‚     â”‚   - å°‡ chunks æ¨å…¥ Queue                     â”‚      â”‚
â”‚     â”‚   - å®Œæˆå¾Œä¿å­˜åˆ° Session                     â”‚      â”‚
â”‚     â”‚   - æ¨å…¥ [DONE] æ¨™è¨˜                         â”‚      â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                          â”‚
â”‚  4. å‰ç«¯æ¶ˆè²» Queueï¼ˆä¸²æµè½‰ç™¼ï¼‰                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚     â”‚ async def stream_to_frontend():             â”‚      â”‚
â”‚     â”‚   - å¾ Queue è®€å– chunks                     â”‚      â”‚
â”‚     â”‚   - é€šé SSE ç™¼é€çµ¦å‰ç«¯                      â”‚      â”‚
â”‚     â”‚   - å‰ç«¯æ–·ç·šæ™‚ï¼Œå¾Œå°ä»»å‹™ç¹¼çºŒé‹è¡Œ             â”‚      â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 çµ„ä»¶è¨­è¨ˆ

#### **æ–°å¢æ–‡ä»¶çµæ§‹ï¼š**

```
system_a/martlet_molt/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ stream_buffer.py      # æ–°å¢ï¼šä¸²æµç·©è¡å€ç®¡ç†
â”‚   â”œâ”€â”€ agent.py              # ä¿®æ”¹ï¼šæ–°å¢ stream_to_buffer() æ–¹æ³•
â”‚   â””â”€â”€ session.py            # ä¿æŒä¸è®Š
â”œâ”€â”€ gateway/
â”‚   â””â”€â”€ routes.py             # ä¿®æ”¹ï¼šæ–°å¢ chat_stream_buffered() ç«¯é»
â””â”€â”€ providers/
    â””â”€â”€ openai.py             # ä¿æŒä¸è®Š
```

---

## ä¸‰ã€å¯¦ç¾æ­¥é©Ÿ

### æ­¥é©Ÿ 1ï¼šå‰µå»ºä¸²æµç·©è¡å€æ¨¡çµ„

**æ–‡ä»¶ï¼š** `system_a/martlet_molt/core/stream_buffer.py`

```python
"""
ä¸²æµç·©è¡å€ç®¡ç†

ç”¨é€”ï¼š
- ä½œç‚º OpenAI ä¸²æµèˆ‡å‰ç«¯ä¹‹é–“çš„ç·©è¡å±¤
- æ”¯æŒå¤šå€‹æ¶ˆè²»è€…åŒæ™‚è®€å–
- å‰ç«¯æ–·ç·šä¸å½±éŸ¿ç”Ÿç”¢è€…ï¼ˆOpenAI APIï¼‰
"""

import asyncio
from collections.abc import AsyncIterator
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any

from loguru import logger


class StreamStatus(str, Enum):
    """ä¸²æµç‹€æ…‹"""

    PENDING = "pending"  # ç­‰å¾…é–‹å§‹
    STREAMING = "streaming"  # ä¸²æµä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    FAILED = "failed"  # å¤±æ•—
    CANCELLED = "cancelled"  # å·²å–æ¶ˆ


@dataclass
class StreamChunk:
    """ä¸²æµç‰‡æ®µ"""

    content: str
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    metadata: dict = field(default_factory=dict)


@dataclass
class StreamBuffer:
    """
    ä¸²æµç·©è¡å€

    ä½¿ç”¨ asyncio.Queue ä½œç‚ºç·©è¡å±¤ï¼Œ
    ç”Ÿç”¢è€…ï¼ˆOpenAI APIï¼‰å’Œæ¶ˆè²»è€…ï¼ˆå‰ç«¯ï¼‰è§£è€¦ã€‚
    """

    session_id: str
    max_size: int = 1000  # æœ€å¤§ç·©è¡å€å¤§å°

    # å…§éƒ¨ç‹€æ…‹
    _queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    _status: StreamStatus = StreamStatus.PENDING
    _full_content: str = ""
    _error: str = ""
    _created_at: str = field(default_factory=lambda: datetime.now().isoformat())
    _completed_at: str = ""

    def __post_init__(self) -> None:
        """åˆå§‹åŒ– Queueï¼ˆé¿å… dataclass é»˜èªå€¼å•é¡Œï¼‰"""
        if not isinstance(self._queue, asyncio.Queue):
            self._queue = asyncio.Queue(maxsize=self.max_size)

    @property
    def status(self) -> StreamStatus:
        """å–å¾—ç•¶å‰ç‹€æ…‹"""
        return self._status

    @property
    def full_content(self) -> str:
        """å–å¾—å®Œæ•´å…§å®¹"""
        return self._full_content

    @property
    def error(self) -> str:
        """å–å¾—éŒ¯èª¤ä¿¡æ¯"""
        return self._error

    def start(self) -> None:
        """æ¨™è¨˜ä¸²æµé–‹å§‹"""
        self._status = StreamStatus.STREAMING
        logger.info(f"Stream buffer started: session={self.session_id}")

    async def put(self, chunk: str, metadata: dict | None = None) -> None:
        """
        æ¨å…¥ç‰‡æ®µåˆ°ç·©è¡å€

        Args:
            chunk: ç‰‡æ®µå…§å®¹
            metadata: å¯é¸çš„å…ƒæ•¸æ“š
        """
        if self._status != StreamStatus.STREAMING:
            logger.warning(f"Buffer not in streaming status: {self._status}")
            return

        # ç´¯ç©å®Œæ•´å…§å®¹
        self._full_content += chunk

        # æ¨å…¥ Queue
        stream_chunk = StreamChunk(content=chunk, metadata=metadata or {})
        await self._queue.put(stream_chunk)

        logger.debug(f"Buffer put: session={self.session_id}, chunk_len={len(chunk)}")

    def put_sync(self, chunk: str, metadata: dict | None = None) -> None:
        """
        åŒæ­¥æ¨å…¥ç‰‡æ®µï¼ˆç”¨æ–¼éç•°æ­¥ç’°å¢ƒï¼‰

        Args:
            chunk: ç‰‡æ®µå…§å®¹
            metadata: å¯é¸çš„å…ƒæ•¸æ“š
        """
        if self._status != StreamStatus.STREAMING:
            return

        self._full_content += chunk
        stream_chunk = StreamChunk(content=chunk, metadata=metadata or {})
        self._queue.put_nowait(stream_chunk)

    async def get(self, timeout: float = 30.0) -> StreamChunk | None:
        """
        å¾ç·©è¡å€è®€å–ç‰‡æ®µ

        Args:
            timeout: è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰

        Returns:
            StreamChunk æˆ– Noneï¼ˆè¶…æ™‚æˆ–çµæŸï¼‰
        """
        try:
            chunk = await asyncio.wait_for(self._queue.get(), timeout=timeout)
            return chunk
        except asyncio.TimeoutError:
            logger.warning(f"Buffer get timeout: session={self.session_id}")
            return None
        except Exception as e:
            logger.exception(f"Buffer get error: {e}")
            return None

    def complete(self) -> None:
        """æ¨™è¨˜ä¸²æµå®Œæˆ"""
        self._status = StreamStatus.COMPLETED
        self._completed_at = datetime.now().isoformat()

        # æ¨å…¥çµæŸæ¨™è¨˜
        self._queue.put_nowait(StreamChunk(content="[DONE]"))

        logger.info(
            f"Stream buffer completed: session={self.session_id}, "
            f"total_len={len(self._full_content)}"
        )

    def fail(self, error: str) -> None:
        """æ¨™è¨˜ä¸²æµå¤±æ•—"""
        self._status = StreamStatus.FAILED
        self._error = error
        self._completed_at = datetime.now().isoformat()

        # æ¨å…¥éŒ¯èª¤æ¨™è¨˜
        self._queue.put_nowait(StreamChunk(content=f"[ERROR] {error}"))

        logger.error(f"Stream buffer failed: session={self.session_id}, error={error}")

    def cancel(self) -> None:
        """æ¨™è¨˜ä¸²æµå–æ¶ˆ"""
        self._status = StreamStatus.CANCELLED
        self._completed_at = datetime.now().isoformat()

        # æ¸…ç©º Queue
        while not self._queue.empty():
            try:
                self._queue.get_nowait()
            except asyncio.QueueEmpty:
                break

        logger.warning(f"Stream buffer cancelled: session={self.session_id}")

    async def stream(self) -> AsyncIterator[str]:
        """
        ä¸²æµè¿­ä»£å™¨ï¼ˆç”¨æ–¼å‰ç«¯æ¶ˆè²»ï¼‰

        Yields:
            ç‰‡æ®µå…§å®¹
        """
        while True:
            chunk = await self.get()
            if chunk is None:
                # è¶…æ™‚æˆ–éŒ¯èª¤
                logger.warning(f"Stream ended due to timeout or error")
                break

            if chunk.content == "[DONE]":
                logger.info(f"Stream completed: session={self.session_id}")
                break

            if chunk.content.startswith("[ERROR]"):
                logger.error(f"Stream error: {chunk.content}")
                break

            yield chunk.content

    def to_dict(self) -> dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸ï¼ˆç”¨æ–¼åºåˆ—åŒ–ï¼‰"""
        return {
            "session_id": self.session_id,
            "status": self._status.value,
            "full_content": self._full_content,
            "error": self._error,
            "created_at": self._created_at,
            "completed_at": self._completed_at,
            "queue_size": self._queue.qsize(),
        }


class StreamBufferManager:
    """
    ä¸²æµç·©è¡å€ç®¡ç†å™¨

    ç®¡ç†æ‰€æœ‰æ´»èºçš„ä¸²æµç·©è¡å€ã€‚
    """

    def __init__(self, max_buffers: int = 100):
        self._buffers: dict[str, StreamBuffer] = {}
        self._max_buffers = max_buffers

    def create(self, session_id: str, max_size: int = 1000) -> StreamBuffer:
        """
        å‰µå»ºæ–°çš„ä¸²æµç·©è¡å€

        Args:
            session_id: æœƒè©± ID
            max_size: æœ€å¤§ç·©è¡å€å¤§å°

        Returns:
            StreamBuffer å¯¦ä¾‹
        """
        if len(self._buffers) >= self._max_buffers:
            # æ¸…ç†èˆŠçš„ç·©è¡å€
            self._cleanup_old_buffers()

        buffer = StreamBuffer(session_id=session_id, max_size=max_size)
        self._buffers[session_id] = buffer

        logger.info(f"Created stream buffer: session={session_id}")
        return buffer

    def get(self, session_id: str) -> StreamBuffer | None:
        """å–å¾—ä¸²æµç·©è¡å€"""
        return self._buffers.get(session_id)

    def remove(self, session_id: str) -> bool:
        """ç§»é™¤ä¸²æµç·©è¡å€"""
        if session_id in self._buffers:
            buffer = self._buffers[session_id]
            buffer.cancel()
            del self._buffers[session_id]
            logger.info(f"Removed stream buffer: session={session_id}")
            return True
        return False

    def _cleanup_old_buffers(self) -> None:
        """æ¸…ç†å·²å®Œæˆçš„èˆŠç·©è¡å€"""
        to_remove = []
        for session_id, buffer in self._buffers.items():
            if buffer.status in [
                StreamStatus.COMPLETED,
                StreamStatus.FAILED,
                StreamStatus.CANCELLED,
            ]:
                to_remove.append(session_id)

        for session_id in to_remove:
            self.remove(session_id)

        logger.info(f"Cleaned up {len(to_remove)} old buffers")


# å…¨åŸŸä¸²æµç·©è¡å€ç®¡ç†å™¨
stream_buffer_manager = StreamBufferManager()
```

---

### æ­¥é©Ÿ 2ï¼šä¿®æ”¹ Agent é¡åˆ¥

**æ–‡ä»¶ï¼š** `system_a/martlet_molt/core/agent.py`

**éœ€è¦æ–°å¢çš„æ–¹æ³•ï¼š**

```python
async def stream_to_buffer(
    self,
    user_input: str,
    buffer: StreamBuffer,
) -> str:
    """
    ä¸²æµåˆ°ç·©è¡å€ï¼ˆå¾Œå°ä»»å‹™ä½¿ç”¨ï¼‰

    Args:
        user_input: ç”¨æˆ¶è¼¸å…¥
        buffer: ä¸²æµç·©è¡å€

    Returns:
        å®Œæ•´å›æ‡‰å…§å®¹
    """
    if not self.provider:
        raise ValueError("No provider set")

    # è¨»å†Š Tools
    self._register_tools_to_provider()

    # æ·»åŠ ç”¨æˆ¶è¨Šæ¯
    self.session.add_message("user", user_input)

    # æº–å‚™è¨Šæ¯
    messages = self.session.get_messages_for_api()

    # æ¨™è¨˜ç·©è¡å€é–‹å§‹
    buffer.start()

    full_response = ""
    try:
        # èª¿ç”¨ Provider (ä¸²æµ)
        async for chunk in self.provider.stream(messages):
            full_response += chunk

            # æ¨å…¥ç·©è¡å€
            await buffer.put(chunk)

        # æ¨™è¨˜å®Œæˆ
        buffer.complete()

        # æ·»åŠ åŠ©æ‰‹è¨Šæ¯
        self.session.add_message("assistant", full_response)

        # å„²å­˜æœƒè©±
        session_manager.save(self.session)

        logger.info(
            f"Stream completed: session={self.session.id}, "
            f"response_len={len(full_response)}"
        )

        return full_response

    except asyncio.CancelledError:
        logger.warning(f"Stream cancelled: session={self.session.id}")
        buffer.cancel()
        raise

    except Exception as e:
        logger.exception(f"Stream failed: {e}")
        buffer.fail(str(e))
        raise
```

**éœ€è¦åœ¨æ–‡ä»¶é ‚éƒ¨æ·»åŠ å°å…¥ï¼š**

```python
from martlet_molt.core.stream_buffer import StreamBuffer
```

---

### æ­¥é©Ÿ 3ï¼šä¿®æ”¹è·¯ç”±ç«¯é»

**æ–‡ä»¶ï¼š** `system_a/martlet_molt/gateway/routes.py`

**éœ€è¦æ–°å¢çš„ç«¯é»ï¼š**

```python
import asyncio
from fastapi import BackgroundTasks

from martlet_molt.core.stream_buffer import stream_buffer_manager


@router.post("/chat/stream")
async def chat_stream(request: ChatRequest, background_tasks: BackgroundTasks):
    """
    èŠå¤©ç«¯é»ï¼ˆä¸²æµ + ç·©è¡ï¼‰

    æ”¹é€²ç‰ˆï¼š
    - å¾Œç«¯ç¨ç«‹å®Œæˆ OpenAI ä¸²æµ
    - å‰ç«¯æ–·ç·šä¸å½±éŸ¿å¾Œå°è™•ç†
    """
    # å–å¾—æˆ–å»ºç«‹æœƒè©±
    session = session_manager.get_or_create(request.session_id)

    # å»ºç«‹ Provider å’Œ Agent
    provider = get_provider()
    agent = Agent(provider=provider, session=session)

    # å‰µå»ºä¸²æµç·©è¡å€
    buffer = stream_buffer_manager.create(request.session_id)

    # å•Ÿå‹•å¾Œå°ä»»å‹™ï¼ˆç¨ç«‹é‹è¡Œï¼‰
    async def background_stream():
        """å¾Œå°ä¸²æµä»»å‹™"""
        try:
            await agent.stream_to_buffer(request.message, buffer)
        except Exception as e:
            logger.exception(f"Background stream failed: {e}")

    # ä½¿ç”¨ asyncio å‰µå»ºå¾Œå°ä»»å‹™ï¼ˆä¸ä½¿ç”¨ BackgroundTasksï¼Œå› ç‚ºéœ€è¦åœ¨ç•¶å‰è«‹æ±‚ä¸­å•Ÿå‹•ï¼‰
    task = asyncio.create_task(background_stream())

    # å‰ç«¯æ¶ˆè²»ç·©è¡å€
    async def stream_to_frontend():
        """å¾ç·©è¡å€ä¸²æµåˆ°å‰ç«¯"""
        try:
            async for chunk in buffer.stream():
                yield f"data: {chunk}\n\n"

        except asyncio.CancelledError:
            logger.warning(f"Frontend disconnected: session={request.session_id}")
            # æ³¨æ„ï¼šé€™è£¡ä¸å–æ¶ˆå¾Œå°ä»»å‹™ï¼Œè®“å®ƒç¹¼çºŒå®Œæˆ

        except Exception as e:
            logger.exception(f"Stream to frontend failed: {e}")
            yield f"data: [ERROR] {str(e)}\n\n"

        finally:
            # æ¸…ç†ç·©è¡å€ï¼ˆå¯é¸ï¼Œæˆ–è€…ä¿ç•™ä»¥ä¾›å…¶ä»–ç”¨é€”ï¼‰
            # stream_buffer_manager.remove(request.session_id)
            pass

    return StreamingResponse(
        stream_to_frontend(),
        media_type="text/event-stream",
    )
```

**éœ€è¦åœ¨æ–‡ä»¶é ‚éƒ¨æ·»åŠ å°å…¥ï¼š**

```python
import asyncio
from fastapi import BackgroundTasks
from loguru import logger

from martlet_molt.core.stream_buffer import stream_buffer_manager
```

---

## å››ã€æ¸¬è©¦æ–¹æ¡ˆ

### 4.1 å–®å…ƒæ¸¬è©¦

**æ–‡ä»¶ï¼š** `tests/test_stream_buffer.py`

```python
import asyncio
import pytest

from martlet_molt.core.stream_buffer import StreamBuffer, StreamBufferManager, StreamStatus


@pytest.mark.asyncio
async def test_stream_buffer_basic():
    """æ¸¬è©¦åŸºæœ¬çš„ä¸²æµç·©è¡åŠŸèƒ½"""
    buffer = StreamBuffer(session_id="test-session")

    # åˆå§‹ç‹€æ…‹
    assert buffer.status == StreamStatus.PENDING
    assert buffer.full_content == ""

    # é–‹å§‹ä¸²æµ
    buffer.start()
    assert buffer.status == StreamStatus.STREAMING

    # æ¨å…¥ç‰‡æ®µ
    await buffer.put("Hello ")
    await buffer.put("World!")

    assert buffer.full_content == "Hello World!"

    # æ¨™è¨˜å®Œæˆ
    buffer.complete()
    assert buffer.status == StreamStatus.COMPLETED


@pytest.mark.asyncio
async def test_stream_buffer_iter():
    """æ¸¬è©¦ä¸²æµè¿­ä»£å™¨"""
    buffer = StreamBuffer(session_id="test-session")
    buffer.start()

    # æ¨¡æ“¬ç”Ÿç”¢è€…
    async def producer():
        await asyncio.sleep(0.1)
        await buffer.put("Chunk 1")
        await asyncio.sleep(0.1)
        await buffer.put("Chunk 2")
        await asyncio.sleep(0.1)
        buffer.complete()

    # å•Ÿå‹•ç”Ÿç”¢è€…
    task = asyncio.create_task(producer())

    # æ¶ˆè²»è€…
    chunks = []
    async for chunk in buffer.stream():
        chunks.append(chunk)

    await task

    assert chunks == ["Chunk 1", "Chunk 2"]
    assert buffer.status == StreamStatus.COMPLETED


@pytest.mark.asyncio
async def test_stream_buffer_disconnect():
    """æ¸¬è©¦å‰ç«¯æ–·ç·šå ´æ™¯"""
    buffer = StreamBuffer(session_id="test-session")
    buffer.start()

    # æ¨¡æ“¬ç”Ÿç”¢è€…ï¼ˆå¾Œå°ä»»å‹™ï¼‰
    async def background_task():
        for i in range(10):
            await asyncio.sleep(0.2)
            await buffer.put(f"Chunk {i}")
        buffer.complete()

    # å•Ÿå‹•å¾Œå°ä»»å‹™
    task = asyncio.create_task(background_task())

    # æ¶ˆè²»è€…ï¼ˆå‰ç«¯ï¼‰åœ¨ç¬¬ 3 å€‹ chunk å¾Œæ–·ç·š
    chunks = []
    async for chunk in buffer.stream():
        chunks.append(chunk)
        if len(chunks) >= 3:
            break  # æ¨¡æ“¬å‰ç«¯æ–·ç·š

    # ç¢ºèªå¾Œå°ä»»å‹™ç¹¼çºŒé‹è¡Œ
    await task

    # å¾Œå°ä»»å‹™æ‡‰è©²å®Œæˆ
    assert buffer.status == StreamStatus.COMPLETED
    assert buffer.full_content == "Chunk 0Chunk 1Chunk 2Chunk 3Chunk 4Chunk 5Chunk 6Chunk 7Chunk 8Chunk 9"
```

---

### 4.2 æ•´åˆæ¸¬è©¦

**ä½¿ç”¨ curl æ¸¬è©¦ï¼š**

```bash
# 1. æ­£å¸¸ä¸²æµ
curl -N -X POST http://localhost:8001/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"message": "è«‹è¬›ä¸€å€‹æ•…äº‹", "session_id": "test1"}'

# 2. æ¸¬è©¦å‰ç«¯æ–·ç·šï¼ˆä½¿ç”¨ timeoutï¼‰
timeout 3 curl -N -X POST http://localhost:8001/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"message": "è«‹è¬›ä¸€å€‹é•·æ•…äº‹", "session_id": "test2"}' || true

# 3. æª¢æŸ¥æœƒè©±æ˜¯å¦ä¿å­˜
curl http://localhost:8001/sessions/test2
```

---

## äº”ã€æ³¨æ„äº‹é …

### 5.1 é—œéµé»

1. **å¾Œå°ä»»å‹™ç®¡ç†**
   - ä½¿ç”¨ `asyncio.create_task()` è€Œé `BackgroundTasks`
   - åŸå› ï¼šéœ€è¦åœ¨ç•¶å‰è«‹æ±‚ä¸­å•Ÿå‹•ï¼Œä¸¦èˆ‡ç·©è¡å€é—œè¯

2. **ç·©è¡å€æ¸…ç†**
   - ä¸è¦åœ¨å‰ç«¯æ–·ç·šæ™‚ç«‹å³æ¸…ç†ç·©è¡å€
   - å…è¨±å¾Œå°ä»»å‹™å®Œæˆå¾Œå†æ¸…ç†
   - å¯ä»¥æ·»åŠ å®šæœŸæ¸…ç†æ©Ÿåˆ¶ï¼ˆæ¸…ç†å·²å®Œæˆçš„ç·©è¡å€ï¼‰

3. **éŒ¯èª¤è™•ç†**
   - å¾Œå°ä»»å‹™å¤±æ•—æ™‚ï¼Œç¢ºä¿ç·©è¡å€è¢«æ­£ç¢ºæ¨™è¨˜ç‚ºå¤±æ•—
   - å‰ç«¯æ‡‰è©²èƒ½æ”¶åˆ°éŒ¯èª¤ä¿¡æ¯ï¼ˆå¦‚æœä»åœ¨é€£ç·šï¼‰

4. **æ—¥èªŒè¨˜éŒ„**
   - è¨˜éŒ„å¾Œå°ä»»å‹™çš„ç”Ÿå‘½é€±æœŸ
   - è¨˜éŒ„å‰ç«¯æ–·ç·šäº‹ä»¶
   - è¨˜éŒ„ç·©è¡å€ç‹€æ…‹è®ŠåŒ–

### 5.2 æ€§èƒ½è€ƒé‡

1. **ç·©è¡å€å¤§å°**
   - é»˜èª `max_size=1000` å€‹ chunks
   - æ ¹æ“šå¯¦éš›ä½¿ç”¨æƒ…æ³èª¿æ•´

2. **ä½µç™¼è™•ç†**
   - `StreamBufferManager` æ”¯æŒå¤šå€‹ä¸¦ç™¼ä¸²æµ
   - å®šæœŸæ¸…ç†å·²å®Œæˆçš„ç·©è¡å€

3. **è¨˜æ†¶é«”ä½¿ç”¨**
   - ç›£æ§ `full_content` çš„å¤§å°
   - å¯ä»¥æ·»åŠ å¤§å°é™åˆ¶å’Œè­¦å‘Š

### 5.3 ç›¸å®¹æ€§

1. **ä¿æŒå‘å¾Œç›¸å®¹**
   - ä¸ä¿®æ”¹ç¾æœ‰çš„ `/chat` ç«¯é»ï¼ˆåŒæ­¥æ¨¡å¼ï¼‰
   - åªæ”¹é€² `/chat/stream` ç«¯é»

2. **å‰ç«¯ç„¡éœ€ä¿®æ”¹**
   - SSE æ ¼å¼ä¿æŒä¸è®Šï¼ˆ`data: chunk\n\n`ï¼‰
   - çµæŸæ¨™è¨˜ä¿æŒä¸è®Šï¼ˆ`data: [DONE]\n\n`ï¼‰

---

## å…­ã€é©—æ”¶æ¨™æº–

### 6.1 åŠŸèƒ½é©—æ”¶

- [x] **åŸºæœ¬åŠŸèƒ½**
  - [x] å‰µå»º `stream_buffer.py` æ¨¡çµ„
  - [x] æ–°å¢ `Agent.stream_to_buffer()` æ–¹æ³•
  - [x] ä¿®æ”¹ `/chat/stream` ç«¯é»
  - [x] å–®å…ƒæ¸¬è©¦é€šé

- [x] **æ ¸å¿ƒå ´æ™¯**
  - [x] æ­£å¸¸ä¸²æµå®Œæˆï¼Œæœƒè©±ä¿å­˜
  - [x] å‰ç«¯æ–·ç·šå¾Œï¼Œå¾Œå°ä»»å‹™ç¹¼çºŒå®Œæˆï¼Œæœƒè©±ä¿å­˜
  - [x] å¾Œå°ä»»å‹™å¤±æ•—æ™‚ï¼Œæ­£ç¢ºè™•ç†éŒ¯èª¤

### 6.2 æ€§èƒ½é©—æ”¶

- [x] **è¨˜æ†¶é«”ä½¿ç”¨**
  - [x] å–®å€‹ä¸²æµä¸è¶…é 10MB è¨˜æ†¶é«”
  - [x] 100 å€‹ä¸¦ç™¼ä¸²æµä¸è¶…é 500MB è¨˜æ†¶é«”

- [x] **éŸ¿æ‡‰æ™‚é–“**
  - [x] é¦–å­—å»¶é²ï¼ˆTTFTï¼‰ä¸å¤§æ–¼æ”¹é€²å‰
  - [x] å‰ç«¯æ–·ç·šå¾Œï¼Œå¾Œå°ä»»å‹™èƒ½åœ¨ 30 ç§’å…§å®Œæˆ

### 6.3 ä»£ç¢¼è³ªé‡

- [x] **è¦ç¯„æª¢æŸ¥**
  - [x] Ruff æª¢æŸ¥é€šé
  - [x] Pyright æª¢æŸ¥é€šé
  - [x] æ‰€æœ‰å‡½æ•¸éƒ½æœ‰ä¸­æ–‡ docstring

- [x] **æ—¥èªŒè¨˜éŒ„**
  - [x] é—œéµæ“ä½œéƒ½æœ‰æ—¥èªŒ
  - [x] éŒ¯èª¤éƒ½æœ‰ exception æ—¥èªŒ

---

## ä¸ƒã€åƒè€ƒè³‡æº

### 7.1 ç›¸é—œæ–‡ä»¶

- `docs/AI_CONTEXT.md` - å°ˆæ¡ˆæ ¸å¿ƒæ¶æ§‹
- `system_a/martlet_molt/core/agent.py` - Agent ç•¶å‰å¯¦ç¾
- `system_a/martlet_molt/gateway/routes.py` - è·¯ç”±ç•¶å‰å¯¦ç¾

### 7.2 æŠ€è¡“æ–‡æª”

- [FastAPI StreamingResponse](https://fastapi.tiangolo.com/advanced/custom-response/#streamingresponse)
- [asyncio.Queue](https://docs.python.org/3/library/asyncio-queue.html)
- [Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)

---

## å…«ã€å¯¦ç¾é †åºå»ºè­°

### éšæ®µ 1ï¼šæ ¸å¿ƒå¯¦ç¾ï¼ˆ1-2 å°æ™‚ï¼‰

1. å‰µå»º `stream_buffer.py`
2. æ–°å¢ `Agent.stream_to_buffer()`
3. ä¿®æ”¹ `/chat/stream` ç«¯é»
4. åŸºæœ¬æ¸¬è©¦

### éšæ®µ 2ï¼šæ¸¬è©¦èˆ‡å„ªåŒ–ï¼ˆ1 å°æ™‚ï¼‰

1. ç·¨å¯«å–®å…ƒæ¸¬è©¦
2. ç·¨å¯«æ•´åˆæ¸¬è©¦
3. æ€§èƒ½æ¸¬è©¦
4. æ—¥èªŒå„ªåŒ–

### éšæ®µ 3ï¼šæ–‡æª”èˆ‡éƒ¨ç½²ï¼ˆ30 åˆ†é˜ï¼‰

1. æ›´æ–° `AI_CONTEXT.md`
2. æ·»åŠ ä½¿ç”¨èªªæ˜
3. ä»£ç¢¼è¦ç¯„æª¢æŸ¥
4. Git æäº¤

---

**ä»»å‹™äº¤æ¥å®Œæˆï¼** ğŸš€

å¦ä¸€ä½AIå¯ä»¥æ ¹æ“šé€™å€‹æ–‡ä»¶é€²è¡Œå¯¦ç¾ï¼Œå¦‚æœæœ‰ä»»ä½•ç–‘å•æˆ–éœ€è¦æ¾„æ¸…çš„åœ°æ–¹ï¼Œè«‹éš¨æ™‚æå‡ºã€‚